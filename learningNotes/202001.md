___Here are the details of January 2020.___

# Date20200101
ä»Šå¤©æ˜¯æ–°å¹´ç¬¬ä¸€å¤©
è¾—è½¬æŠ˜è…¾ï¼Œ2019å¹´å·²è½ä¸‹å¸·å¹•ï¼Œå¥½å§ï¼Œé‚£å°±è®©å¥‡è¿¹åœ¨2020å¹´ä»æ­¤åˆ»å¼€å§‹è§è¯â€¦â€¦
## ä»Šæ—¥å°ç»“
### softmaxå›å½’ä»é›¶å¼€å§‹å®ç°ï¼š
1. è·å–å’Œè¯»å–æ•°æ®;
2. åˆå§‹åŒ–æ¨¡å‹å‚æ•°ï¼šè®¾ç½®è¾“å…¥ã€åç½®ã€è¾“å‡ºå‚æ•°å¤§å°ï¼›
3. å®ç°softmaxå‡½æ•°è¿ç®—æ–¹æ³•ï¼›
4. å®šä¹‰æ¨¡å‹ï¼šä¹Ÿå³ç®—æ³•çš„è®¡ç®—å…¬å¼ä»£ç åŒ–ï¼›
5. å®šä¹‰æŸå¤±å‡½æ•°ï¼š
6. è®¡ç®—åˆ†ç±»å‡†ç¡®ç‡ï¼šç”±äºæ ‡ç­¾ç±»å‹ä¸ºæ•´æ•°ï¼Œè¿™é‡Œå¯èƒ½ä¼šç”¨åˆ°.mean().asscalar()å‡½æ•°ï¼ŒX.asscalar()æ˜¯å°†å‘é‡Xè½¬æ¢æˆæ ‡é‡ï¼Œä¸”å‘é‡Xåªèƒ½ä¸ºä¸€ç»´å«å•ä¸ªå…ƒç´ çš„å‘é‡;
7. è®­ç»ƒæ¨¡å‹ï¼›
```python
# æœ¬å‡½æ•°å·²ä¿å­˜åœ¨d2lzhåŒ…ä¸­æ–¹ä¾¿ä»¥åä½¿ç”¨
def train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size,
              params=None, lr=None, trainer=None):
    for epoch in range(num_epochs):
        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0
        for X, y in train_iter:
            with autograd.record():
                y_hat = net(X)
                l = loss(y_hat, y).sum()
            l.backward()
            if trainer is None:
                d2l.sgd(params, lr, batch_size)
            else:
                trainer.step(batch_size)  # â€œsoftmaxå›å½’çš„ç®€æ´å®ç°â€ä¸€èŠ‚å°†ç”¨åˆ°
            y = y.astype('float32')
            train_l_sum += l.asscalar()
            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().asscalar()
            n += y.size
        test_acc = evaluate_accuracy(test_iter, net)
        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'
              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))
```
8. é¢„æµ‹ã€‚

### MLPå¤šå±‚æ„ŸçŸ¥æœºä»é›¶å¼€å§‹å®ç°ï¼š
1. è·å–å’Œè¯»å–æ•°æ®;
2. å®šä¹‰æ¨¡å‹å‚æ•°ï¼šè®¾ç½®è¾“å…¥å±‚ã€éšè—å±‚ã€è¾“å‡ºå±‚å‚æ•°ï¼›
3. å®šä¹‰æ¿€æ´»å‡½æ•°ï¼›
4. å®šä¹‰æŸå¤±å‡½æ•°ï¼›
5. è®­ç»ƒæ¨¡å‹ï¼›

# Date20200102

## ä»Šæ—¥å°ç»“
### MLPå¤šå±‚æ„ŸçŸ¥æœºä½¿ç”¨Gluonæ¥å®ç°
1. è·å–å’Œè¯»å–æ•°æ®ï¼›
2. å®šä¹‰æ¨¡å‹ï¼šç›´æ¥è°ƒgluonç°æœ‰æ¥å£ï¼Œä¼ å‚å³å¯ï¼Œæ¿€æ´»å‡½æ•°ä¹Ÿä½œä¸ºå‚æ•°å®šä¹‰ï¼›
```python
net = nn.Sequential()
net.add(nn.Dense(256, activation='relu'), nn.Dense(10))
net.initialize(init.Normal(sigma=0.01))
```
3. å®šä¹‰æŸå¤±å‡½æ•°ï¼šç›´æ¥è°ƒgluonæ¥å£ï¼Œloss = gloss.SoftmaxCrossEntropyLoss();
4. è®­ç»ƒæ¨¡å‹ï¼›

# Date20200103

## ä»Šæ—¥å°ç»“
### æ¬ æ‹Ÿåˆã€è¿‡æ‹Ÿåˆã€æ¨¡å‹é€‰æ‹©
* ç”±äºæ— æ³•ä»è®­ç»ƒè¯¯å·®ä¼°è®¡æ³›åŒ–è¯¯å·®ï¼Œä¸€å‘³åœ°é™ä½è®­ç»ƒè¯¯å·®å¹¶ä¸æ„å‘³ç€æ³›åŒ–è¯¯å·®ä¸€å®šä¼šé™ä½ã€‚æœºå™¨å­¦ä¹ æ¨¡å‹åº”å…³æ³¨é™ä½æ³›åŒ–è¯¯å·®ã€‚
* å¯ä»¥ä½¿ç”¨éªŒè¯æ•°æ®é›†æ¥è¿›è¡Œæ¨¡å‹é€‰æ‹©ã€‚
* æ¬ æ‹ŸåˆæŒ‡æ¨¡å‹æ— æ³•å¾—åˆ°è¾ƒä½çš„è®­ç»ƒè¯¯å·®ï¼Œè¿‡æ‹ŸåˆæŒ‡æ¨¡å‹çš„è®­ç»ƒè¯¯å·®è¿œå°äºå®ƒåœ¨æµ‹è¯•æ•°æ®é›†ä¸Šçš„è¯¯å·®ã€‚
* åº”é€‰æ‹©å¤æ‚åº¦åˆé€‚çš„æ¨¡å‹å¹¶é¿å…ä½¿ç”¨è¿‡å°‘çš„è®­ç»ƒæ ·æœ¬ã€‚

# Date20200104
ä»Šæ—¥ä¼‘æ¯

# Date20200106
## ä»Šæ—¥å°ç»“
### æƒé‡è¡°å‡ï¼ˆweight decayï¼‰
æƒé‡è¡°å‡ç­‰ä»·äº ğ¿2 èŒƒæ•°æ­£åˆ™åŒ–ï¼ˆregularizationï¼‰ã€‚æ­£åˆ™åŒ–é€šè¿‡ä¸ºæ¨¡å‹æŸå¤±å‡½æ•°æ·»åŠ æƒ©ç½šé¡¹ä½¿å­¦å‡ºçš„æ¨¡å‹å‚æ•°å€¼è¾ƒå°ï¼Œæ˜¯åº”å¯¹è¿‡æ‹Ÿåˆçš„å¸¸ç”¨æ‰‹æ®µã€‚



